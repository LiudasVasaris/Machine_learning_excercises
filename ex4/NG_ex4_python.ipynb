{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['X'].T, data['y'].T\n",
    "y[y==10] = 0 # set zeros equal to 0, not 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a):\n",
    "    \"\"\"One hot encode variable and return its transpose: k x M , k-features, M-examples\"\"\"\n",
    "    one_hot_encoded = np.zeros( (a.size, len(np.unique(a))) )\n",
    "    one_hot_encoded[np.arange(a.size), a] = 1\n",
    "    return one_hot_encoded.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to one-hot encode our output to vector\n",
    "y_matrix = one_hot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "output_layer_size = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat('ex4/ex4weights.mat')\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note all shapes for easier coding:\n",
    "1. X - 400,5000\n",
    "1. y - 10,5000\n",
    "1. W1 - 25,400\n",
    "1. b1 - 25,1\n",
    "1. W2 - 10,25\n",
    "1. b2 - 10,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(W1, b1, W2, b2, X):\n",
    "    \"\"\"feed forward nn. Returns of all layer values\"\"\"\n",
    "    a1 = X\n",
    "    a2 = W1@a1 + b1\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = W2@a2 + b2\n",
    "    z3 = sigmoid(a3)\n",
    "    return a1, a2, z2, a3, z3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards_propagations(a1, a2, z2, a3, z3, W1, W2, y, lambda_=0):\n",
    "    \"\"\"backwards propogation of nn\"\"\"\n",
    "    m = y.shape[1]\n",
    "    dZ3 = z3 - y # We find how much we missed predictions by\n",
    "    \n",
    "    dW2 = 1/m * dZ3@(a2.T) + (lambda_/m)*W2 ## add the regularization term\n",
    "                           # We multiply how wrong our answers were(dZ3) by what values we got(a2),\n",
    "                           # and get the needed adjustment. If we want to regularize, we add the term\n",
    "    db2 = 1/m * np.sum(dZ3, 1) # We adjust the weight of bias based on average needed to account \n",
    "    db2 = np.reshape(db2,(db2.size,1)) # for wrong answers(dZ3). We also reshape to retain 2d matrix\n",
    "    dZ2 =  W2.T@dZ3 * sigmoid_grad(z2) # We back propagate final error by multiplying it with\n",
    "                                       # incoming weights to get second layer error and adjust with\n",
    "                                       # cost function derivative\n",
    "    # Other adjustments follow the same principle\n",
    "    dW1 = 1/m * dZ2@(a1.T) + (lambda_/m)*W1\n",
    "    db1 = 1/m * np.sum(dZ2, 1)\n",
    "    db1 = np.reshape(db1,(db1.size,1))         \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(matrix_shapes):\n",
    "    return [np.random.rand(i,j) for i,j in matrix_shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, weight_deltas, alpha):\n",
    "    return [w - wd*alpha for w, wd in zip(weights, weight_deltas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(z3):\n",
    "    return np.argmax(z3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(z3,y):\n",
    "    return np.sum(prediction(z3) == y) / y.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_wrong_example_mask(z3,y):\n",
    "    return prediction(z3) != y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y, iterations, alpha=0.1, lambda_=0, X_test = None, y_test = None):\n",
    "    W1, b1, W2, b2 = init_weights([(hidden_layer_size, input_layer_size), (hidden_layer_size,1),\n",
    "                                  (output_layer_size,hidden_layer_size), (output_layer_size,1)])\n",
    "    y_one_hot = one_hot(y)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        y_test_one_hot = one_hot(y_test)\n",
    "    \n",
    "    accuracy_hist = []\n",
    "    test_hist = []\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        a1, a2, z2, a3, z3 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backwards_propagations(a1, a2, z2, a3, z3, W1, W2, y_one_hot, lambda_)\n",
    "        W1, b1, W2, b2 = update_weights([W1, b1, W2, b2], [dW1, db1, dW2, db2], alpha)\n",
    "        \n",
    "        \n",
    "        if i%100==0:\n",
    "            print(f\"Iteration {i}, Train Accuracy: {calculate_accuracy(z3,y)}\")\n",
    "            accuracy_hist.append(calculate_accuracy(z3,y))\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_params = forward_propagation(W1, b1, W2, b2, X_test)\n",
    "                print(f\"Test Accuracy: {calculate_accuracy(test_params[4],y_test)}\")\n",
    "                test_hist.append(calculate_accuracy(test_params[4],y_test))\n",
    "            \n",
    "            \n",
    "    return W1, b1, W2, b2, [accuracy_hist,test_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_w_cost_func(weights, X,y, alpha=0.1, lambda_=0):\n",
    "    epsilon = 1e-5\n",
    "    m = y.size\n",
    "    W1, b1, W2, b2 = unflatten_weights(weights)\n",
    "    y_one_hot = one_hot(y)\n",
    "    a1, a2, z2, a3, z3 = forward_propagation(W1, b1, W2, b2, X)\n",
    "    dW1, db1, dW2, db2 = backwards_propagations(a1, a2, z2, a3, z3,W1, W2, y_one_hot)\n",
    "    W1, b1, W2, b2 = update_weights([W1, b1, W2, b2], [dW1, db1, dW2, db2], alpha)\n",
    "    reg_term = lambda_/(2*m) * ( np.sum(np.square(W1)) + np.sum(np.square(W2)) )\n",
    "\n",
    "    J = (-1 / m) * np.sum( (np.log(z3+epsilon) * y_one_hot) + np.log(1 - z3+epsilon) * (1 - y_one_hot) ) + reg_term\n",
    "    return J, np.concatenate([w.flatten() for w in [W1, b1, W2, b2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_weights(flat_W):\n",
    "    temp_w = flat_W[:input_layer_size*hidden_layer_size]\n",
    "    W1 = np.reshape(temp_w,(hidden_layer_size,input_layer_size))\n",
    "    temp_w =  flat_W[input_layer_size*hidden_layer_size:]\n",
    "    b1 = np.reshape(temp_w[:hidden_layer_size],(hidden_layer_size,1))\n",
    "    temp_w =  temp_w[hidden_layer_size:]\n",
    "    W2 = np.reshape(temp_w[:hidden_layer_size*output_layer_size],(output_layer_size,hidden_layer_size))\n",
    "    temp_w = temp_w[hidden_layer_size*output_layer_size:]\n",
    "    b2 = np.reshape(temp_w[:output_layer_size],(output_layer_size,1))\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_weights = init_weights([(hidden_layer_size, input_layer_size), (hidden_layer_size,1),\n",
    "                                  (output_layer_size,hidden_layer_size), (output_layer_size,1)])\n",
    "flat_weights = np.concatenate([w.flatten() for w in start_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = [samp.T for samp in train_test_split(X.T, y.T, test_size=0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "options= {'maxiter': 2000}\n",
    "lambda_gd = lambda w : gradient_descent_w_cost_func(w, X_train, y_train)\n",
    "res = minimize(lambda_gd,\n",
    "                        flat_weights,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "weights_fin = nn_params\n",
    "tained_weights_cost = unflatten_weights(weights_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Accuracy: 0.1\n",
      "Test Accuracy: 0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-7b358eb7760f>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Train Accuracy: 0.4795\n",
      "Test Accuracy: 0.479\n",
      "Iteration 200, Train Accuracy: 0.66425\n",
      "Test Accuracy: 0.633\n",
      "Iteration 300, Train Accuracy: 0.797\n",
      "Test Accuracy: 0.716\n",
      "Iteration 400, Train Accuracy: 0.83325\n",
      "Test Accuracy: 0.786\n",
      "Iteration 500, Train Accuracy: 0.8455\n",
      "Test Accuracy: 0.796\n",
      "Iteration 600, Train Accuracy: 0.856\n",
      "Test Accuracy: 0.801\n",
      "Iteration 700, Train Accuracy: 0.85875\n",
      "Test Accuracy: 0.83\n",
      "Iteration 800, Train Accuracy: 0.8685\n",
      "Test Accuracy: 0.846\n",
      "Iteration 900, Train Accuracy: 0.87025\n",
      "Test Accuracy: 0.85\n",
      "Iteration 1000, Train Accuracy: 0.8745\n",
      "Test Accuracy: 0.854\n",
      "Iteration 1100, Train Accuracy: 0.87875\n",
      "Test Accuracy: 0.859\n",
      "Iteration 1200, Train Accuracy: 0.8835\n",
      "Test Accuracy: 0.865\n",
      "Iteration 1300, Train Accuracy: 0.88725\n",
      "Test Accuracy: 0.867\n",
      "Iteration 1400, Train Accuracy: 0.88975\n",
      "Test Accuracy: 0.872\n",
      "Iteration 1500, Train Accuracy: 0.892\n",
      "Test Accuracy: 0.873\n",
      "Iteration 1600, Train Accuracy: 0.89625\n",
      "Test Accuracy: 0.876\n",
      "Iteration 1700, Train Accuracy: 0.89775\n",
      "Test Accuracy: 0.878\n",
      "Iteration 1800, Train Accuracy: 0.89975\n",
      "Test Accuracy: 0.881\n",
      "Iteration 1900, Train Accuracy: 0.902\n",
      "Test Accuracy: 0.881\n",
      "Iteration 2000, Train Accuracy: 0.904\n",
      "Test Accuracy: 0.883\n",
      "Iteration 2100, Train Accuracy: 0.9055\n",
      "Test Accuracy: 0.883\n",
      "Iteration 2200, Train Accuracy: 0.9075\n",
      "Test Accuracy: 0.886\n",
      "Iteration 2300, Train Accuracy: 0.90925\n",
      "Test Accuracy: 0.885\n",
      "Iteration 2400, Train Accuracy: 0.91\n",
      "Test Accuracy: 0.886\n",
      "Iteration 2500, Train Accuracy: 0.911\n",
      "Test Accuracy: 0.886\n",
      "Iteration 2600, Train Accuracy: 0.91375\n",
      "Test Accuracy: 0.885\n",
      "Iteration 2700, Train Accuracy: 0.91525\n",
      "Test Accuracy: 0.886\n",
      "Iteration 2800, Train Accuracy: 0.916\n",
      "Test Accuracy: 0.886\n",
      "Iteration 2900, Train Accuracy: 0.91675\n",
      "Test Accuracy: 0.887\n",
      "Iteration 3000, Train Accuracy: 0.918\n",
      "Test Accuracy: 0.889\n",
      "Iteration 3100, Train Accuracy: 0.91875\n",
      "Test Accuracy: 0.89\n",
      "Iteration 3200, Train Accuracy: 0.9195\n",
      "Test Accuracy: 0.888\n",
      "Iteration 3300, Train Accuracy: 0.92025\n",
      "Test Accuracy: 0.89\n",
      "Iteration 3400, Train Accuracy: 0.92075\n",
      "Test Accuracy: 0.891\n",
      "Iteration 3500, Train Accuracy: 0.921\n",
      "Test Accuracy: 0.892\n",
      "Iteration 3600, Train Accuracy: 0.9215\n",
      "Test Accuracy: 0.892\n",
      "Iteration 3700, Train Accuracy: 0.9225\n",
      "Test Accuracy: 0.892\n",
      "Iteration 3800, Train Accuracy: 0.9225\n",
      "Test Accuracy: 0.892\n",
      "Iteration 3900, Train Accuracy: 0.9225\n",
      "Test Accuracy: 0.892\n",
      "Iteration 4000, Train Accuracy: 0.92375\n",
      "Test Accuracy: 0.892\n",
      "Iteration 4100, Train Accuracy: 0.92325\n",
      "Test Accuracy: 0.891\n",
      "Iteration 4200, Train Accuracy: 0.92475\n",
      "Test Accuracy: 0.891\n",
      "Iteration 4300, Train Accuracy: 0.92525\n",
      "Test Accuracy: 0.893\n",
      "Iteration 4400, Train Accuracy: 0.92525\n",
      "Test Accuracy: 0.893\n",
      "Iteration 4500, Train Accuracy: 0.926\n",
      "Test Accuracy: 0.894\n",
      "Iteration 4600, Train Accuracy: 0.92725\n",
      "Test Accuracy: 0.894\n",
      "Iteration 4700, Train Accuracy: 0.92825\n",
      "Test Accuracy: 0.895\n",
      "Iteration 4800, Train Accuracy: 0.92875\n",
      "Test Accuracy: 0.896\n",
      "Iteration 4900, Train Accuracy: 0.92925\n",
      "Test Accuracy: 0.896\n",
      "Iteration 5000, Train Accuracy: 0.92925\n",
      "Test Accuracy: 0.897\n",
      "Iteration 5100, Train Accuracy: 0.9305\n",
      "Test Accuracy: 0.897\n",
      "Iteration 5200, Train Accuracy: 0.93025\n",
      "Test Accuracy: 0.896\n",
      "Iteration 5300, Train Accuracy: 0.93075\n",
      "Test Accuracy: 0.895\n",
      "Iteration 5400, Train Accuracy: 0.9315\n",
      "Test Accuracy: 0.895\n",
      "Iteration 5500, Train Accuracy: 0.93175\n",
      "Test Accuracy: 0.895\n",
      "Iteration 5600, Train Accuracy: 0.932\n",
      "Test Accuracy: 0.895\n",
      "Iteration 5700, Train Accuracy: 0.93175\n",
      "Test Accuracy: 0.895\n",
      "Iteration 5800, Train Accuracy: 0.93225\n",
      "Test Accuracy: 0.895\n",
      "Iteration 5900, Train Accuracy: 0.93275\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6000, Train Accuracy: 0.933\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6100, Train Accuracy: 0.933\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6200, Train Accuracy: 0.9335\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6300, Train Accuracy: 0.9345\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6400, Train Accuracy: 0.935\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6500, Train Accuracy: 0.93525\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6600, Train Accuracy: 0.935\n",
      "Test Accuracy: 0.895\n",
      "Iteration 6700, Train Accuracy: 0.93575\n",
      "Test Accuracy: 0.896\n",
      "Iteration 6800, Train Accuracy: 0.93575\n",
      "Test Accuracy: 0.896\n",
      "Iteration 6900, Train Accuracy: 0.93625\n",
      "Test Accuracy: 0.896\n",
      "Iteration 7000, Train Accuracy: 0.93625\n",
      "Test Accuracy: 0.896\n",
      "Iteration 7100, Train Accuracy: 0.93625\n",
      "Test Accuracy: 0.897\n",
      "Iteration 7200, Train Accuracy: 0.93625\n",
      "Test Accuracy: 0.897\n",
      "Iteration 7300, Train Accuracy: 0.93625\n",
      "Test Accuracy: 0.896\n",
      "Iteration 7400, Train Accuracy: 0.9365\n",
      "Test Accuracy: 0.896\n",
      "Iteration 7500, Train Accuracy: 0.9365\n",
      "Test Accuracy: 0.896\n",
      "Iteration 7600, Train Accuracy: 0.9365\n",
      "Test Accuracy: 0.896\n",
      "Iteration 7700, Train Accuracy: 0.93675\n",
      "Test Accuracy: 0.897\n",
      "Iteration 7800, Train Accuracy: 0.93725\n",
      "Test Accuracy: 0.898\n",
      "Iteration 7900, Train Accuracy: 0.9375\n",
      "Test Accuracy: 0.898\n",
      "Iteration 8000, Train Accuracy: 0.93725\n",
      "Test Accuracy: 0.898\n",
      "Iteration 8100, Train Accuracy: 0.9375\n",
      "Test Accuracy: 0.899\n",
      "Iteration 8200, Train Accuracy: 0.9375\n",
      "Test Accuracy: 0.9\n",
      "Iteration 8300, Train Accuracy: 0.9375\n",
      "Test Accuracy: 0.899\n",
      "Iteration 8400, Train Accuracy: 0.9375\n",
      "Test Accuracy: 0.9\n",
      "Iteration 8500, Train Accuracy: 0.93775\n",
      "Test Accuracy: 0.9\n",
      "Iteration 8600, Train Accuracy: 0.93825\n",
      "Test Accuracy: 0.9\n",
      "Iteration 8700, Train Accuracy: 0.93825\n",
      "Test Accuracy: 0.9\n",
      "Iteration 8800, Train Accuracy: 0.93875\n",
      "Test Accuracy: 0.9\n",
      "Iteration 8900, Train Accuracy: 0.9395\n",
      "Test Accuracy: 0.9\n",
      "Iteration 9000, Train Accuracy: 0.94\n",
      "Test Accuracy: 0.9\n",
      "Iteration 9100, Train Accuracy: 0.94025\n",
      "Test Accuracy: 0.9\n",
      "Iteration 9200, Train Accuracy: 0.94025\n",
      "Test Accuracy: 0.9\n",
      "Iteration 9300, Train Accuracy: 0.9405\n",
      "Test Accuracy: 0.9\n",
      "Iteration 9400, Train Accuracy: 0.94075\n",
      "Test Accuracy: 0.9\n",
      "Iteration 9500, Train Accuracy: 0.9405\n",
      "Test Accuracy: 0.899\n",
      "Iteration 9600, Train Accuracy: 0.9405\n",
      "Test Accuracy: 0.899\n",
      "Iteration 9700, Train Accuracy: 0.94025\n",
      "Test Accuracy: 0.899\n",
      "Iteration 9800, Train Accuracy: 0.94025\n",
      "Test Accuracy: 0.899\n",
      "Iteration 9900, Train Accuracy: 0.9405\n",
      "Test Accuracy: 0.899\n",
      "------------------------\n",
      "Iteration 0, Train Accuracy: 0.1\n",
      "Test Accuracy: 0.1\n",
      "Iteration 100, Train Accuracy: 0.43175\n",
      "Test Accuracy: 0.345\n",
      "Iteration 200, Train Accuracy: 0.70275\n",
      "Test Accuracy: 0.63\n",
      "Iteration 300, Train Accuracy: 0.792\n",
      "Test Accuracy: 0.753\n",
      "Iteration 400, Train Accuracy: 0.78125\n",
      "Test Accuracy: 0.804\n",
      "Iteration 500, Train Accuracy: 0.8265\n",
      "Test Accuracy: 0.789\n",
      "Iteration 600, Train Accuracy: 0.83425\n",
      "Test Accuracy: 0.813\n",
      "Iteration 700, Train Accuracy: 0.85125\n",
      "Test Accuracy: 0.842\n",
      "Iteration 800, Train Accuracy: 0.8585\n",
      "Test Accuracy: 0.85\n",
      "Iteration 900, Train Accuracy: 0.864\n",
      "Test Accuracy: 0.855\n",
      "Iteration 1000, Train Accuracy: 0.86975\n",
      "Test Accuracy: 0.859\n",
      "Iteration 1100, Train Accuracy: 0.874\n",
      "Test Accuracy: 0.867\n",
      "Iteration 1200, Train Accuracy: 0.87925\n",
      "Test Accuracy: 0.87\n",
      "Iteration 1300, Train Accuracy: 0.8835\n",
      "Test Accuracy: 0.873\n",
      "Iteration 1400, Train Accuracy: 0.8865\n",
      "Test Accuracy: 0.876\n",
      "Iteration 1500, Train Accuracy: 0.8895\n",
      "Test Accuracy: 0.88\n",
      "Iteration 1600, Train Accuracy: 0.89175\n",
      "Test Accuracy: 0.881\n",
      "Iteration 1700, Train Accuracy: 0.89425\n",
      "Test Accuracy: 0.882\n",
      "Iteration 1800, Train Accuracy: 0.897\n",
      "Test Accuracy: 0.881\n",
      "Iteration 1900, Train Accuracy: 0.901\n",
      "Test Accuracy: 0.882\n",
      "Iteration 2000, Train Accuracy: 0.90275\n",
      "Test Accuracy: 0.882\n",
      "Iteration 2100, Train Accuracy: 0.904\n",
      "Test Accuracy: 0.883\n",
      "Iteration 2200, Train Accuracy: 0.9045\n",
      "Test Accuracy: 0.884\n",
      "Iteration 2300, Train Accuracy: 0.90675\n",
      "Test Accuracy: 0.885\n",
      "Iteration 2400, Train Accuracy: 0.90775\n",
      "Test Accuracy: 0.885\n",
      "Iteration 2500, Train Accuracy: 0.9095\n",
      "Test Accuracy: 0.886\n",
      "Iteration 2600, Train Accuracy: 0.90925\n",
      "Test Accuracy: 0.889\n",
      "Iteration 2700, Train Accuracy: 0.91025\n",
      "Test Accuracy: 0.888\n",
      "Iteration 2800, Train Accuracy: 0.9115\n",
      "Test Accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "tained_weights_and_history = gradient_descent(X_train,y_train,10000,0.1,0,X_test,y_test)\n",
    "print('------------------------')\n",
    "tained_weights_and_history_regularized = gradient_descent(X_train,y_train,10000,0.1,1,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained with minimization from Sci-kit and cost function\n",
    "# prediction_1 = forward_propagation(*tained_weights_cost,X_test)\n",
    "# accuracy_1 = calculate_accuracy(prediction_1[4],y_test)\n",
    "# print(f\"Model trained with minimization scored {accuracy_1} on test set\")\n",
    "# For some reasone it does not work\n",
    "*trained_weight_1, history_of_accuracy_1 = tained_weights_and_history\n",
    "prediction_1 = forward_propagation(*trained_weight_1,X_test)\n",
    "accuracy_1 = calculate_accuracy(prediction_1[4],y_test)\n",
    "print(f\"Model trained with manual gradient descent {accuracy_1} on test set\")\n",
    "*trained_weight_2, history_of_accuracy_2 = tained_weights_and_history_regularized\n",
    "prediction_2 = forward_propagation(*trained_weight_2,X_test)\n",
    "accuracy_2= calculate_accuracy(prediction_2[4],y_test)\n",
    "print(f\"Model trained with manual gradient descent and regularization {accuracy_1} on test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_of_accuracy[0])\n",
    "plt.plot(history_of_accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a high accuracy and no decrease in validation(test) set after any number of itterations, thus we can conclude, that the model is propperly trained and does not overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the examples the model fails to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_examples = return_wrong_example_mask(prediction_1[4],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_prediction_input = X_test[:,wrong_examples.reshape(-1)].T\n",
    "wrong_prediction_output = y_test[:,wrong_examples.reshape(-1)].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_prediction(i, o):\n",
    "    sample = np.random.randint(low=0, high=len(o))\n",
    "    i_samp, o_samp = i[sample], o[sample]\n",
    "    example = np.reshape(i_samp,(20,20))\n",
    "    plt.imshow(example)\n",
    "    print(o_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "see_prediction(wrong_prediction_input,wrong_prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some examples, we can clearly see that not even a human could verify that the written number corresponds to the actual value\n",
    "\n",
    "We either have incorrect labels for some examples or corrupt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
